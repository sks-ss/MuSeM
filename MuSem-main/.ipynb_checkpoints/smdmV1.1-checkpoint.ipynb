{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6aaefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.idea',\n",
       " '.ipynb_checkpoints',\n",
       " 'data_directory',\n",
       " 'first.ipynb',\n",
       " 'glove.6B.300d.txt',\n",
       " 'MuSem_V1',\n",
       " 'myscript.py',\n",
       " 'New_headline_gen',\n",
       " 'README.md',\n",
       " 'test.csv',\n",
       " 'test_dataset',\n",
       " 'train.csv',\n",
       " 'train_dataset_300',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import os\n",
    "#full path.\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "files = os.listdir(os.getcwd())\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12dd7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load csv files\n"
     ]
    }
   ],
   "source": [
    "print(\"Load csv files\")\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import re, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6556314a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This week , explained : spies , special counse...</td>\n",
       "      <td>The Russia investigation got real this week . ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jolyon Palmer to leave Renault after Japanese ...</td>\n",
       "      <td>British driver Jolyon Palmer is to leave the R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Former FBi Director James Comey Admits That He...</td>\n",
       "      <td>Liberals have been frothing at the mouth , wai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump : ' I have n't changed my stance ' on China</td>\n",
       "      <td>President Trump said Tuesday that he has n't s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Announces New Cabinet Member With Choice...</td>\n",
       "      <td>Newsmax reported Trump ’ s next move in the st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  This week , explained : spies , special counse...   \n",
       "1  Jolyon Palmer to leave Renault after Japanese ...   \n",
       "2  Former FBi Director James Comey Admits That He...   \n",
       "3  Trump : ' I have n't changed my stance ' on China   \n",
       "4  Trump Announces New Cabinet Member With Choice...   \n",
       "\n",
       "                                           synthetic  label  \n",
       "0  The Russia investigation got real this week . ...      0  \n",
       "1  British driver Jolyon Palmer is to leave the R...      1  \n",
       "2  Liberals have been frothing at the mouth , wai...      0  \n",
       "3  President Trump said Tuesday that he has n't s...      1  \n",
       "4  Newsmax reported Trump ’ s next move in the st...      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28aa8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"./\"\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_file_path = data_directory+\"/train_dataset_300\"\n",
    "test_dataset_file_path = data_directory+\"/test_dataset_300\"\n",
    "\n",
    "create_test_dataset = False\n",
    "\n",
    "\n",
    "train_file_path = data_directory+\"/train.csv\"\n",
    "test_file_path = data_directory+\"/test.csv\"\n",
    "\n",
    "\n",
    "embedding_file_path = data_directory+\"/glove.6B.300d.txt\"\n",
    "test_dataset_file_path = data_directory+\"/test_dataset\"\n",
    "\n",
    "\n",
    "embedding_dim = 300\n",
    "max_sen_len = 50\n",
    "\n",
    "X_train = []\n",
    "X_train_lenght = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "X_test_lenght = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c90fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_file_path, newline='',encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        X_train.append([row[i] for i in [0, 1]])\n",
    "        X_train_lenght.append([len(row[i]) for i in [0, 1]])\n",
    "        y_train.append(row[2])\n",
    "X_train = X_train[1:]\n",
    "X_train_lenght = X_train_lenght[1:]\n",
    "y_train = y_train[1:]\n",
    "with open(test_file_path, newline='',encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        X_test.append([row[i] for i in [0, 1]])\n",
    "        X_test_lenght.append([len(row[i]) for i in [0, 1]])\n",
    "X_test = X_test[1:]\n",
    "X_test_lenght = X_test_lenght[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04b90d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '0', '1', '0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2f394ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing\n"
     ]
    }
   ],
   "source": [
    "print(\"data preprocessing\")\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(sample):\n",
    "    words = nltk.word_tokenize(sample)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    return words\n",
    "\n",
    "def load_embedding(embedding_file_path, wordset, embedding_dim):\n",
    "    words_dict = dict()\n",
    "    word_embedding = []\n",
    "    index = 1\n",
    "    words_dict['$EOF$'] = 0\n",
    "    word_embedding.append(np.zeros(embedding_dim))\n",
    "    with open(embedding_file_path, 'r',encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            check = line.strip().split()\n",
    "            if len(check) == 2: continue\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in wordset: continue\n",
    "            embedding = np.array([float(s) for s in line[1:]])\n",
    "            word_embedding.append(embedding)\n",
    "            words_dict[line[0]] = index\n",
    "            index +=1\n",
    "    return word_embedding, words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18676871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 852\n",
      "1 / 852\n",
      "2 / 852\n",
      "3 / 852\n",
      "4 / 852\n",
      "5 / 852\n",
      "6 / 852\n",
      "7 / 852\n",
      "8 / 852\n",
      "9 / 852\n",
      "10 / 852\n",
      "11 / 852\n",
      "12 / 852\n",
      "13 / 852\n",
      "14 / 852\n",
      "15 / 852\n",
      "16 / 852\n",
      "17 / 852\n",
      "18 / 852\n",
      "19 / 852\n",
      "20 / 852\n",
      "21 / 852\n",
      "22 / 852\n",
      "23 / 852\n",
      "24 / 852\n",
      "25 / 852\n",
      "26 / 852\n",
      "27 / 852\n",
      "28 / 852\n",
      "29 / 852\n",
      "30 / 852\n",
      "31 / 852\n",
      "32 / 852\n",
      "33 / 852\n",
      "34 / 852\n",
      "35 / 852\n",
      "36 / 852\n",
      "37 / 852\n",
      "38 / 852\n",
      "39 / 852\n",
      "40 / 852\n",
      "41 / 852\n",
      "42 / 852\n",
      "43 / 852\n",
      "44 / 852\n",
      "45 / 852\n",
      "46 / 852\n",
      "47 / 852\n",
      "48 / 852\n",
      "49 / 852\n",
      "50 / 852\n",
      "51 / 852\n",
      "52 / 852\n",
      "53 / 852\n",
      "54 / 852\n",
      "55 / 852\n",
      "56 / 852\n",
      "57 / 852\n",
      "58 / 852\n",
      "59 / 852\n",
      "60 / 852\n",
      "61 / 852\n",
      "62 / 852\n",
      "63 / 852\n",
      "64 / 852\n",
      "65 / 852\n",
      "66 / 852\n",
      "67 / 852\n",
      "68 / 852\n",
      "69 / 852\n",
      "70 / 852\n",
      "71 / 852\n",
      "72 / 852\n",
      "73 / 852\n",
      "74 / 852\n",
      "75 / 852\n",
      "76 / 852\n",
      "77 / 852\n",
      "78 / 852\n",
      "79 / 852\n",
      "80 / 852\n",
      "81 / 852\n",
      "82 / 852\n",
      "83 / 852\n",
      "84 / 852\n",
      "85 / 852\n",
      "86 / 852\n",
      "87 / 852\n",
      "88 / 852\n",
      "89 / 852\n",
      "90 / 852\n",
      "91 / 852\n",
      "92 / 852\n",
      "93 / 852\n",
      "94 / 852\n",
      "95 / 852\n",
      "96 / 852\n",
      "97 / 852\n",
      "98 / 852\n",
      "99 / 852\n",
      "100 / 852\n",
      "101 / 852\n",
      "102 / 852\n",
      "103 / 852\n",
      "104 / 852\n",
      "105 / 852\n",
      "106 / 852\n",
      "107 / 852\n",
      "108 / 852\n",
      "109 / 852\n",
      "110 / 852\n",
      "111 / 852\n",
      "112 / 852\n",
      "113 / 852\n",
      "114 / 852\n",
      "115 / 852\n",
      "116 / 852\n",
      "117 / 852\n",
      "118 / 852\n",
      "119 / 852\n",
      "120 / 852\n",
      "121 / 852\n",
      "122 / 852\n",
      "123 / 852\n",
      "124 / 852\n",
      "125 / 852\n",
      "126 / 852\n",
      "127 / 852\n",
      "128 / 852\n",
      "129 / 852\n",
      "130 / 852\n",
      "131 / 852\n",
      "132 / 852\n",
      "133 / 852\n",
      "134 / 852\n",
      "135 / 852\n",
      "136 / 852\n",
      "137 / 852\n",
      "138 / 852\n",
      "139 / 852\n",
      "140 / 852\n",
      "141 / 852\n",
      "142 / 852\n",
      "143 / 852\n",
      "144 / 852\n",
      "145 / 852\n",
      "146 / 852\n",
      "147 / 852\n",
      "148 / 852\n",
      "149 / 852\n",
      "150 / 852\n",
      "151 / 852\n",
      "152 / 852\n",
      "153 / 852\n",
      "154 / 852\n",
      "155 / 852\n",
      "156 / 852\n",
      "157 / 852\n",
      "158 / 852\n",
      "159 / 852\n",
      "160 / 852\n",
      "161 / 852\n",
      "162 / 852\n",
      "163 / 852\n",
      "164 / 852\n",
      "165 / 852\n",
      "166 / 852\n",
      "167 / 852\n",
      "168 / 852\n",
      "169 / 852\n",
      "170 / 852\n",
      "171 / 852\n",
      "172 / 852\n",
      "173 / 852\n",
      "174 / 852\n",
      "175 / 852\n",
      "176 / 852\n",
      "177 / 852\n",
      "178 / 852\n",
      "179 / 852\n",
      "180 / 852\n",
      "181 / 852\n",
      "182 / 852\n",
      "183 / 852\n",
      "184 / 852\n",
      "185 / 852\n",
      "186 / 852\n",
      "187 / 852\n",
      "188 / 852\n",
      "189 / 852\n",
      "190 / 852\n",
      "191 / 852\n",
      "192 / 852\n",
      "193 / 852\n",
      "194 / 852\n",
      "195 / 852\n",
      "196 / 852\n",
      "197 / 852\n",
      "198 / 852\n",
      "199 / 852\n",
      "200 / 852\n",
      "201 / 852\n",
      "202 / 852\n",
      "203 / 852\n",
      "204 / 852\n",
      "205 / 852\n",
      "206 / 852\n",
      "207 / 852\n",
      "208 / 852\n",
      "209 / 852\n",
      "210 / 852\n",
      "211 / 852\n",
      "212 / 852\n",
      "213 / 852\n",
      "214 / 852\n",
      "215 / 852\n",
      "216 / 852\n",
      "217 / 852\n",
      "218 / 852\n",
      "219 / 852\n",
      "220 / 852\n",
      "221 / 852\n",
      "222 / 852\n",
      "223 / 852\n",
      "224 / 852\n",
      "225 / 852\n",
      "226 / 852\n",
      "227 / 852\n",
      "228 / 852\n",
      "229 / 852\n",
      "230 / 852\n",
      "231 / 852\n",
      "232 / 852\n",
      "233 / 852\n",
      "234 / 852\n",
      "235 / 852\n",
      "236 / 852\n",
      "237 / 852\n",
      "238 / 852\n",
      "239 / 852\n",
      "240 / 852\n",
      "241 / 852\n",
      "242 / 852\n",
      "243 / 852\n",
      "244 / 852\n",
      "245 / 852\n",
      "246 / 852\n",
      "247 / 852\n",
      "248 / 852\n",
      "249 / 852\n",
      "250 / 852\n",
      "251 / 852\n",
      "252 / 852\n",
      "253 / 852\n",
      "254 / 852\n",
      "255 / 852\n",
      "256 / 852\n",
      "257 / 852\n",
      "258 / 852\n",
      "259 / 852\n",
      "260 / 852\n",
      "261 / 852\n",
      "262 / 852\n",
      "263 / 852\n",
      "264 / 852\n",
      "265 / 852\n",
      "266 / 852\n",
      "267 / 852\n",
      "268 / 852\n",
      "269 / 852\n",
      "270 / 852\n",
      "271 / 852\n",
      "272 / 852\n",
      "273 / 852\n",
      "274 / 852\n",
      "275 / 852\n",
      "276 / 852\n",
      "277 / 852\n",
      "278 / 852\n",
      "279 / 852\n",
      "280 / 852\n",
      "281 / 852\n",
      "282 / 852\n",
      "283 / 852\n",
      "284 / 852\n",
      "285 / 852\n",
      "286 / 852\n",
      "287 / 852\n",
      "288 / 852\n",
      "289 / 852\n",
      "290 / 852\n",
      "291 / 852\n",
      "292 / 852\n",
      "293 / 852\n",
      "294 / 852\n",
      "295 / 852\n",
      "296 / 852\n",
      "297 / 852\n",
      "298 / 852\n",
      "299 / 852\n",
      "300 / 852\n",
      "301 / 852\n",
      "302 / 852\n",
      "303 / 852\n",
      "304 / 852\n",
      "305 / 852\n",
      "306 / 852\n",
      "307 / 852\n",
      "308 / 852\n",
      "309 / 852\n",
      "310 / 852\n",
      "311 / 852\n",
      "312 / 852\n",
      "313 / 852\n",
      "314 / 852\n",
      "315 / 852\n",
      "316 / 852\n",
      "317 / 852\n",
      "318 / 852\n",
      "319 / 852\n",
      "320 / 852\n",
      "321 / 852\n",
      "322 / 852\n",
      "323 / 852\n",
      "324 / 852\n",
      "325 / 852\n",
      "326 / 852\n",
      "327 / 852\n",
      "328 / 852\n",
      "329 / 852\n",
      "330 / 852\n",
      "331 / 852\n",
      "332 / 852\n",
      "333 / 852\n",
      "334 / 852\n",
      "335 / 852\n",
      "336 / 852\n",
      "337 / 852\n",
      "338 / 852\n",
      "339 / 852\n",
      "340 / 852\n",
      "341 / 852\n",
      "342 / 852\n",
      "343 / 852\n",
      "344 / 852\n",
      "345 / 852\n",
      "346 / 852\n",
      "347 / 852\n",
      "348 / 852\n",
      "349 / 852\n",
      "350 / 852\n",
      "351 / 852\n",
      "352 / 852\n",
      "353 / 852\n",
      "354 / 852\n",
      "355 / 852\n",
      "356 / 852\n",
      "357 / 852\n",
      "358 / 852\n",
      "359 / 852\n",
      "360 / 852\n",
      "361 / 852\n",
      "362 / 852\n",
      "363 / 852\n",
      "364 / 852\n",
      "365 / 852\n",
      "366 / 852\n",
      "367 / 852\n",
      "368 / 852\n",
      "369 / 852\n",
      "370 / 852\n",
      "371 / 852\n",
      "372 / 852\n",
      "373 / 852\n",
      "374 / 852\n",
      "375 / 852\n",
      "376 / 852\n",
      "377 / 852\n",
      "378 / 852\n",
      "379 / 852\n",
      "380 / 852\n",
      "381 / 852\n",
      "382 / 852\n",
      "383 / 852\n",
      "384 / 852\n",
      "385 / 852\n",
      "386 / 852\n",
      "387 / 852\n",
      "388 / 852\n",
      "389 / 852\n",
      "390 / 852\n",
      "391 / 852\n",
      "392 / 852\n",
      "393 / 852\n",
      "394 / 852\n",
      "395 / 852\n",
      "396 / 852\n",
      "397 / 852\n",
      "398 / 852\n",
      "399 / 852\n",
      "400 / 852\n",
      "401 / 852\n",
      "402 / 852\n",
      "403 / 852\n",
      "404 / 852\n",
      "405 / 852\n",
      "406 / 852\n",
      "407 / 852\n",
      "408 / 852\n",
      "409 / 852\n",
      "410 / 852\n",
      "411 / 852\n",
      "412 / 852\n",
      "413 / 852\n",
      "414 / 852\n",
      "415 / 852\n",
      "416 / 852\n",
      "417 / 852\n",
      "418 / 852\n",
      "419 / 852\n",
      "420 / 852\n",
      "421 / 852\n",
      "422 / 852\n",
      "423 / 852\n",
      "424 / 852\n",
      "425 / 852\n",
      "426 / 852\n",
      "427 / 852\n",
      "428 / 852\n",
      "429 / 852\n",
      "430 / 852\n",
      "431 / 852\n",
      "432 / 852\n",
      "433 / 852\n",
      "434 / 852\n",
      "435 / 852\n",
      "436 / 852\n",
      "437 / 852\n",
      "438 / 852\n",
      "439 / 852\n",
      "440 / 852\n",
      "441 / 852\n",
      "442 / 852\n",
      "443 / 852\n",
      "444 / 852\n",
      "445 / 852\n",
      "446 / 852\n",
      "447 / 852\n",
      "448 / 852\n",
      "449 / 852\n",
      "450 / 852\n",
      "451 / 852\n",
      "452 / 852\n",
      "453 / 852\n",
      "454 / 852\n",
      "455 / 852\n",
      "456 / 852\n",
      "457 / 852\n",
      "458 / 852\n",
      "459 / 852\n",
      "460 / 852\n",
      "461 / 852\n",
      "462 / 852\n",
      "463 / 852\n",
      "464 / 852\n",
      "465 / 852\n",
      "466 / 852\n",
      "467 / 852\n",
      "468 / 852\n",
      "469 / 852\n",
      "470 / 852\n",
      "471 / 852\n",
      "472 / 852\n",
      "473 / 852\n",
      "474 / 852\n",
      "475 / 852\n",
      "476 / 852\n",
      "477 / 852\n",
      "478 / 852\n",
      "479 / 852\n",
      "480 / 852\n",
      "481 / 852\n",
      "482 / 852\n",
      "483 / 852\n",
      "484 / 852\n",
      "485 / 852\n",
      "486 / 852\n",
      "487 / 852\n",
      "488 / 852\n",
      "489 / 852\n",
      "490 / 852\n",
      "491 / 852\n",
      "492 / 852\n",
      "493 / 852\n",
      "494 / 852\n",
      "495 / 852\n",
      "496 / 852\n",
      "497 / 852\n",
      "498 / 852\n",
      "499 / 852\n",
      "500 / 852\n",
      "501 / 852\n",
      "502 / 852\n",
      "503 / 852\n",
      "504 / 852\n",
      "505 / 852\n",
      "506 / 852\n",
      "507 / 852\n",
      "508 / 852\n",
      "509 / 852\n",
      "510 / 852\n",
      "511 / 852\n",
      "512 / 852\n",
      "513 / 852\n",
      "514 / 852\n",
      "515 / 852\n",
      "516 / 852\n",
      "517 / 852\n",
      "518 / 852\n",
      "519 / 852\n",
      "520 / 852\n",
      "521 / 852\n",
      "522 / 852\n",
      "523 / 852\n",
      "524 / 852\n",
      "525 / 852\n",
      "526 / 852\n",
      "527 / 852\n",
      "528 / 852\n",
      "529 / 852\n",
      "530 / 852\n",
      "531 / 852\n",
      "532 / 852\n",
      "533 / 852\n",
      "534 / 852\n",
      "535 / 852\n",
      "536 / 852\n",
      "537 / 852\n",
      "538 / 852\n",
      "539 / 852\n",
      "540 / 852\n",
      "541 / 852\n",
      "542 / 852\n",
      "543 / 852\n",
      "544 / 852\n",
      "545 / 852\n",
      "546 / 852\n",
      "547 / 852\n",
      "548 / 852\n",
      "549 / 852\n",
      "550 / 852\n",
      "551 / 852\n",
      "552 / 852\n",
      "553 / 852\n",
      "554 / 852\n",
      "555 / 852\n",
      "556 / 852\n",
      "557 / 852\n",
      "558 / 852\n",
      "559 / 852\n",
      "560 / 852\n",
      "561 / 852\n",
      "562 / 852\n",
      "563 / 852\n",
      "564 / 852\n",
      "565 / 852\n",
      "566 / 852\n",
      "567 / 852\n",
      "568 / 852\n",
      "569 / 852\n",
      "570 / 852\n",
      "571 / 852\n",
      "572 / 852\n",
      "573 / 852\n",
      "574 / 852\n",
      "575 / 852\n",
      "576 / 852\n",
      "577 / 852\n",
      "578 / 852\n",
      "579 / 852\n",
      "580 / 852\n",
      "581 / 852\n",
      "582 / 852\n",
      "583 / 852\n",
      "584 / 852\n",
      "585 / 852\n",
      "586 / 852\n",
      "587 / 852\n",
      "588 / 852\n",
      "589 / 852\n",
      "590 / 852\n",
      "591 / 852\n",
      "592 / 852\n",
      "593 / 852\n",
      "594 / 852\n",
      "595 / 852\n",
      "596 / 852\n",
      "597 / 852\n",
      "598 / 852\n",
      "599 / 852\n",
      "600 / 852\n",
      "601 / 852\n",
      "602 / 852\n",
      "603 / 852\n",
      "604 / 852\n",
      "605 / 852\n",
      "606 / 852\n",
      "607 / 852\n",
      "608 / 852\n",
      "609 / 852\n",
      "610 / 852\n",
      "611 / 852\n",
      "612 / 852\n",
      "613 / 852\n",
      "614 / 852\n",
      "615 / 852\n",
      "616 / 852\n",
      "617 / 852\n",
      "618 / 852\n",
      "619 / 852\n",
      "620 / 852\n",
      "621 / 852\n",
      "622 / 852\n",
      "623 / 852\n",
      "624 / 852\n",
      "625 / 852\n",
      "626 / 852\n",
      "627 / 852\n",
      "628 / 852\n",
      "629 / 852\n",
      "630 / 852\n",
      "631 / 852\n",
      "632 / 852\n",
      "633 / 852\n",
      "634 / 852\n",
      "635 / 852\n",
      "636 / 852\n",
      "637 / 852\n",
      "638 / 852\n",
      "639 / 852\n",
      "640 / 852\n",
      "641 / 852\n",
      "642 / 852\n",
      "643 / 852\n",
      "644 / 852\n",
      "645 / 852\n",
      "646 / 852\n",
      "647 / 852\n",
      "648 / 852\n",
      "649 / 852\n",
      "650 / 852\n",
      "651 / 852\n",
      "652 / 852\n",
      "653 / 852\n",
      "654 / 852\n",
      "655 / 852\n",
      "656 / 852\n",
      "657 / 852\n",
      "658 / 852\n",
      "659 / 852\n",
      "660 / 852\n",
      "661 / 852\n",
      "662 / 852\n",
      "663 / 852\n",
      "664 / 852\n",
      "665 / 852\n",
      "666 / 852\n",
      "667 / 852\n",
      "668 / 852\n",
      "669 / 852\n",
      "670 / 852\n",
      "671 / 852\n",
      "672 / 852\n",
      "673 / 852\n",
      "674 / 852\n",
      "675 / 852\n",
      "676 / 852\n",
      "677 / 852\n",
      "678 / 852\n",
      "679 / 852\n",
      "680 / 852\n",
      "681 / 852\n",
      "682 / 852\n",
      "683 / 852\n",
      "684 / 852\n",
      "685 / 852\n",
      "686 / 852\n",
      "687 / 852\n",
      "688 / 852\n",
      "689 / 852\n",
      "690 / 852\n",
      "691 / 852\n",
      "692 / 852\n",
      "693 / 852\n",
      "694 / 852\n",
      "695 / 852\n",
      "696 / 852\n",
      "697 / 852\n",
      "698 / 852\n",
      "699 / 852\n",
      "700 / 852\n",
      "701 / 852\n",
      "702 / 852\n",
      "703 / 852\n",
      "704 / 852\n",
      "705 / 852\n",
      "706 / 852\n",
      "707 / 852\n",
      "708 / 852\n",
      "709 / 852\n",
      "710 / 852\n",
      "711 / 852\n",
      "712 / 852\n",
      "713 / 852\n",
      "714 / 852\n",
      "715 / 852\n",
      "716 / 852\n",
      "717 / 852\n",
      "718 / 852\n",
      "719 / 852\n",
      "720 / 852\n",
      "721 / 852\n",
      "722 / 852\n",
      "723 / 852\n",
      "724 / 852\n",
      "725 / 852\n",
      "726 / 852\n",
      "727 / 852\n",
      "728 / 852\n",
      "729 / 852\n",
      "730 / 852\n",
      "731 / 852\n",
      "732 / 852\n",
      "733 / 852\n",
      "734 / 852\n",
      "735 / 852\n",
      "736 / 852\n",
      "737 / 852\n",
      "738 / 852\n",
      "739 / 852\n",
      "740 / 852\n",
      "741 / 852\n",
      "742 / 852\n",
      "743 / 852\n",
      "744 / 852\n",
      "745 / 852\n",
      "746 / 852\n",
      "747 / 852\n",
      "748 / 852\n",
      "749 / 852\n",
      "750 / 852\n",
      "751 / 852\n",
      "752 / 852\n",
      "753 / 852\n",
      "754 / 852\n",
      "755 / 852\n",
      "756 / 852\n",
      "757 / 852\n",
      "758 / 852\n",
      "759 / 852\n",
      "760 / 852\n",
      "761 / 852\n",
      "762 / 852\n",
      "763 / 852\n",
      "764 / 852\n",
      "765 / 852\n",
      "766 / 852\n",
      "767 / 852\n",
      "768 / 852\n",
      "769 / 852\n",
      "770 / 852\n",
      "771 / 852\n",
      "772 / 852\n",
      "773 / 852\n",
      "774 / 852\n",
      "775 / 852\n",
      "776 / 852\n",
      "777 / 852\n",
      "778 / 852\n",
      "779 / 852\n",
      "780 / 852\n",
      "781 / 852\n",
      "782 / 852\n",
      "783 / 852\n",
      "784 / 852\n",
      "785 / 852\n",
      "786 / 852\n",
      "787 / 852\n",
      "788 / 852\n",
      "789 / 852\n",
      "790 / 852\n",
      "791 / 852\n",
      "792 / 852\n",
      "793 / 852\n",
      "794 / 852\n",
      "795 / 852\n",
      "796 / 852\n",
      "797 / 852\n",
      "798 / 852\n",
      "799 / 852\n",
      "800 / 852\n",
      "801 / 852\n",
      "802 / 852\n",
      "803 / 852\n",
      "804 / 852\n",
      "805 / 852\n",
      "806 / 852\n",
      "807 / 852\n",
      "808 / 852\n",
      "809 / 852\n",
      "810 / 852\n",
      "811 / 852\n",
      "812 / 852\n",
      "813 / 852\n",
      "814 / 852\n",
      "815 / 852\n",
      "816 / 852\n",
      "817 / 852\n",
      "818 / 852\n",
      "819 / 852\n",
      "820 / 852\n",
      "821 / 852\n",
      "822 / 852\n",
      "823 / 852\n",
      "824 / 852\n",
      "825 / 852\n",
      "826 / 852\n",
      "827 / 852\n",
      "828 / 852\n",
      "829 / 852\n",
      "830 / 852\n",
      "831 / 852\n",
      "832 / 852\n",
      "833 / 852\n",
      "834 / 852\n",
      "835 / 852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836 / 852\n",
      "837 / 852\n",
      "838 / 852\n",
      "839 / 852\n",
      "840 / 852\n",
      "841 / 852\n",
      "842 / 852\n",
      "843 / 852\n",
      "844 / 852\n",
      "845 / 852\n",
      "846 / 852\n",
      "847 / 852\n",
      "848 / 852\n",
      "849 / 852\n",
      "850 / 852\n",
      "851 / 852\n",
      "0 / 852\n",
      "1 / 852\n",
      "2 / 852\n",
      "3 / 852\n",
      "4 / 852\n",
      "5 / 852\n",
      "6 / 852\n",
      "7 / 852\n",
      "8 / 852\n",
      "9 / 852\n",
      "10 / 852\n",
      "11 / 852\n",
      "12 / 852\n",
      "13 / 852\n",
      "14 / 852\n",
      "15 / 852\n",
      "16 / 852\n",
      "17 / 852\n",
      "18 / 852\n",
      "19 / 852\n",
      "20 / 852\n",
      "21 / 852\n",
      "22 / 852\n",
      "23 / 852\n",
      "24 / 852\n",
      "25 / 852\n",
      "26 / 852\n",
      "27 / 852\n",
      "28 / 852\n",
      "29 / 852\n",
      "30 / 852\n",
      "31 / 852\n",
      "32 / 852\n",
      "33 / 852\n",
      "34 / 852\n",
      "35 / 852\n",
      "36 / 852\n",
      "37 / 852\n",
      "38 / 852\n",
      "39 / 852\n",
      "40 / 852\n",
      "41 / 852\n",
      "42 / 852\n",
      "43 / 852\n",
      "44 / 852\n",
      "45 / 852\n",
      "46 / 852\n",
      "47 / 852\n",
      "48 / 852\n",
      "49 / 852\n",
      "50 / 852\n",
      "51 / 852\n",
      "52 / 852\n",
      "53 / 852\n",
      "54 / 852\n",
      "55 / 852\n",
      "56 / 852\n",
      "57 / 852\n",
      "58 / 852\n",
      "59 / 852\n",
      "60 / 852\n",
      "61 / 852\n",
      "62 / 852\n",
      "63 / 852\n",
      "64 / 852\n",
      "65 / 852\n",
      "66 / 852\n",
      "67 / 852\n",
      "68 / 852\n",
      "69 / 852\n",
      "70 / 852\n",
      "71 / 852\n",
      "72 / 852\n",
      "73 / 852\n",
      "74 / 852\n",
      "75 / 852\n",
      "76 / 852\n",
      "77 / 852\n",
      "78 / 852\n",
      "79 / 852\n",
      "80 / 852\n",
      "81 / 852\n",
      "82 / 852\n",
      "83 / 852\n",
      "84 / 852\n",
      "85 / 852\n",
      "86 / 852\n",
      "87 / 852\n",
      "88 / 852\n",
      "89 / 852\n",
      "90 / 852\n",
      "91 / 852\n",
      "92 / 852\n",
      "93 / 852\n",
      "94 / 852\n",
      "95 / 852\n",
      "96 / 852\n",
      "97 / 852\n",
      "98 / 852\n",
      "99 / 852\n",
      "100 / 852\n",
      "101 / 852\n",
      "102 / 852\n",
      "103 / 852\n",
      "104 / 852\n",
      "105 / 852\n",
      "106 / 852\n",
      "107 / 852\n",
      "108 / 852\n",
      "109 / 852\n",
      "110 / 852\n",
      "111 / 852\n",
      "112 / 852\n",
      "113 / 852\n",
      "114 / 852\n",
      "115 / 852\n",
      "116 / 852\n",
      "117 / 852\n",
      "118 / 852\n",
      "119 / 852\n",
      "120 / 852\n",
      "121 / 852\n",
      "122 / 852\n",
      "123 / 852\n",
      "124 / 852\n",
      "125 / 852\n",
      "126 / 852\n",
      "127 / 852\n",
      "128 / 852\n",
      "129 / 852\n",
      "130 / 852\n",
      "131 / 852\n",
      "132 / 852\n",
      "133 / 852\n",
      "134 / 852\n",
      "135 / 852\n",
      "136 / 852\n",
      "137 / 852\n",
      "138 / 852\n",
      "139 / 852\n",
      "140 / 852\n",
      "141 / 852\n",
      "142 / 852\n",
      "143 / 852\n",
      "144 / 852\n",
      "145 / 852\n",
      "146 / 852\n",
      "147 / 852\n",
      "148 / 852\n",
      "149 / 852\n",
      "150 / 852\n",
      "151 / 852\n",
      "152 / 852\n",
      "153 / 852\n",
      "154 / 852\n",
      "155 / 852\n",
      "156 / 852\n",
      "157 / 852\n",
      "158 / 852\n",
      "159 / 852\n",
      "160 / 852\n",
      "161 / 852\n",
      "162 / 852\n",
      "163 / 852\n",
      "164 / 852\n",
      "165 / 852\n",
      "166 / 852\n",
      "167 / 852\n",
      "168 / 852\n",
      "169 / 852\n",
      "170 / 852\n",
      "171 / 852\n",
      "172 / 852\n",
      "173 / 852\n",
      "174 / 852\n",
      "175 / 852\n",
      "176 / 852\n",
      "177 / 852\n",
      "178 / 852\n",
      "179 / 852\n",
      "180 / 852\n",
      "181 / 852\n",
      "182 / 852\n",
      "183 / 852\n",
      "184 / 852\n",
      "185 / 852\n",
      "186 / 852\n",
      "187 / 852\n",
      "188 / 852\n",
      "189 / 852\n",
      "190 / 852\n",
      "191 / 852\n",
      "192 / 852\n",
      "193 / 852\n",
      "194 / 852\n",
      "195 / 852\n",
      "196 / 852\n",
      "197 / 852\n",
      "198 / 852\n",
      "199 / 852\n",
      "200 / 852\n",
      "201 / 852\n",
      "202 / 852\n",
      "203 / 852\n",
      "204 / 852\n",
      "205 / 852\n",
      "206 / 852\n",
      "207 / 852\n",
      "208 / 852\n",
      "209 / 852\n",
      "210 / 852\n",
      "211 / 852\n",
      "212 / 852\n",
      "213 / 852\n",
      "214 / 852\n",
      "215 / 852\n",
      "216 / 852\n",
      "217 / 852\n",
      "218 / 852\n",
      "219 / 852\n",
      "220 / 852\n",
      "221 / 852\n",
      "222 / 852\n",
      "223 / 852\n",
      "224 / 852\n",
      "225 / 852\n",
      "226 / 852\n",
      "227 / 852\n",
      "228 / 852\n",
      "229 / 852\n",
      "230 / 852\n",
      "231 / 852\n",
      "232 / 852\n",
      "233 / 852\n",
      "234 / 852\n",
      "235 / 852\n",
      "236 / 852\n",
      "237 / 852\n",
      "238 / 852\n",
      "239 / 852\n",
      "240 / 852\n",
      "241 / 852\n",
      "242 / 852\n",
      "243 / 852\n",
      "244 / 852\n",
      "245 / 852\n",
      "246 / 852\n",
      "247 / 852\n",
      "248 / 852\n",
      "249 / 852\n",
      "250 / 852\n",
      "251 / 852\n",
      "252 / 852\n",
      "253 / 852\n",
      "254 / 852\n",
      "255 / 852\n",
      "256 / 852\n",
      "257 / 852\n",
      "258 / 852\n",
      "259 / 852\n",
      "260 / 852\n",
      "261 / 852\n",
      "262 / 852\n",
      "263 / 852\n",
      "264 / 852\n",
      "265 / 852\n",
      "266 / 852\n",
      "267 / 852\n",
      "268 / 852\n",
      "269 / 852\n",
      "270 / 852\n",
      "271 / 852\n",
      "272 / 852\n",
      "273 / 852\n",
      "274 / 852\n",
      "275 / 852\n",
      "276 / 852\n",
      "277 / 852\n",
      "278 / 852\n",
      "279 / 852\n",
      "280 / 852\n",
      "281 / 852\n",
      "282 / 852\n",
      "283 / 852\n",
      "284 / 852\n",
      "285 / 852\n",
      "286 / 852\n",
      "287 / 852\n",
      "288 / 852\n",
      "289 / 852\n",
      "290 / 852\n",
      "291 / 852\n",
      "292 / 852\n",
      "293 / 852\n",
      "294 / 852\n",
      "295 / 852\n",
      "296 / 852\n",
      "297 / 852\n",
      "298 / 852\n",
      "299 / 852\n",
      "300 / 852\n",
      "301 / 852\n",
      "302 / 852\n",
      "303 / 852\n",
      "304 / 852\n",
      "305 / 852\n",
      "306 / 852\n",
      "307 / 852\n",
      "308 / 852\n",
      "309 / 852\n",
      "310 / 852\n",
      "311 / 852\n",
      "312 / 852\n",
      "313 / 852\n",
      "314 / 852\n",
      "315 / 852\n",
      "316 / 852\n",
      "317 / 852\n",
      "318 / 852\n",
      "319 / 852\n",
      "320 / 852\n",
      "321 / 852\n",
      "322 / 852\n",
      "323 / 852\n",
      "324 / 852\n",
      "325 / 852\n",
      "326 / 852\n",
      "327 / 852\n",
      "328 / 852\n",
      "329 / 852\n",
      "330 / 852\n",
      "331 / 852\n",
      "332 / 852\n",
      "333 / 852\n",
      "334 / 852\n",
      "335 / 852\n",
      "336 / 852\n",
      "337 / 852\n",
      "338 / 852\n",
      "339 / 852\n",
      "340 / 852\n",
      "341 / 852\n",
      "342 / 852\n",
      "343 / 852\n",
      "344 / 852\n",
      "345 / 852\n",
      "346 / 852\n",
      "347 / 852\n",
      "348 / 852\n",
      "349 / 852\n",
      "350 / 852\n",
      "351 / 852\n",
      "352 / 852\n",
      "353 / 852\n",
      "354 / 852\n",
      "355 / 852\n",
      "356 / 852\n",
      "357 / 852\n",
      "358 / 852\n",
      "359 / 852\n",
      "360 / 852\n",
      "361 / 852\n",
      "362 / 852\n",
      "363 / 852\n",
      "364 / 852\n",
      "365 / 852\n",
      "366 / 852\n",
      "367 / 852\n",
      "368 / 852\n",
      "369 / 852\n",
      "370 / 852\n",
      "371 / 852\n",
      "372 / 852\n",
      "373 / 852\n",
      "374 / 852\n",
      "375 / 852\n",
      "376 / 852\n",
      "377 / 852\n",
      "378 / 852\n",
      "379 / 852\n",
      "380 / 852\n",
      "381 / 852\n",
      "382 / 852\n",
      "383 / 852\n",
      "384 / 852\n",
      "385 / 852\n",
      "386 / 852\n",
      "387 / 852\n",
      "388 / 852\n",
      "389 / 852\n",
      "390 / 852\n",
      "391 / 852\n",
      "392 / 852\n",
      "393 / 852\n",
      "394 / 852\n",
      "395 / 852\n",
      "396 / 852\n",
      "397 / 852\n",
      "398 / 852\n",
      "399 / 852\n",
      "400 / 852\n",
      "401 / 852\n",
      "402 / 852\n",
      "403 / 852\n",
      "404 / 852\n",
      "405 / 852\n",
      "406 / 852\n",
      "407 / 852\n",
      "408 / 852\n",
      "409 / 852\n",
      "410 / 852\n",
      "411 / 852\n",
      "412 / 852\n",
      "413 / 852\n",
      "414 / 852\n",
      "415 / 852\n",
      "416 / 852\n",
      "417 / 852\n",
      "418 / 852\n",
      "419 / 852\n",
      "420 / 852\n",
      "421 / 852\n",
      "422 / 852\n",
      "423 / 852\n",
      "424 / 852\n",
      "425 / 852\n",
      "426 / 852\n",
      "427 / 852\n",
      "428 / 852\n",
      "429 / 852\n",
      "430 / 852\n",
      "431 / 852\n",
      "432 / 852\n",
      "433 / 852\n",
      "434 / 852\n",
      "435 / 852\n",
      "436 / 852\n",
      "437 / 852\n",
      "438 / 852\n",
      "439 / 852\n",
      "440 / 852\n",
      "441 / 852\n",
      "442 / 852\n",
      "443 / 852\n",
      "444 / 852\n",
      "445 / 852\n",
      "446 / 852\n",
      "447 / 852\n",
      "448 / 852\n",
      "449 / 852\n",
      "450 / 852\n",
      "451 / 852\n",
      "452 / 852\n",
      "453 / 852\n",
      "454 / 852\n",
      "455 / 852\n",
      "456 / 852\n",
      "457 / 852\n",
      "458 / 852\n",
      "459 / 852\n",
      "460 / 852\n",
      "461 / 852\n",
      "462 / 852\n",
      "463 / 852\n",
      "464 / 852\n",
      "465 / 852\n",
      "466 / 852\n",
      "467 / 852\n",
      "468 / 852\n",
      "469 / 852\n",
      "470 / 852\n",
      "471 / 852\n",
      "472 / 852\n",
      "473 / 852\n",
      "474 / 852\n",
      "475 / 852\n",
      "476 / 852\n",
      "477 / 852\n",
      "478 / 852\n",
      "479 / 852\n",
      "480 / 852\n",
      "481 / 852\n",
      "482 / 852\n",
      "483 / 852\n",
      "484 / 852\n",
      "485 / 852\n",
      "486 / 852\n",
      "487 / 852\n",
      "488 / 852\n",
      "489 / 852\n",
      "490 / 852\n",
      "491 / 852\n",
      "492 / 852\n",
      "493 / 852\n",
      "494 / 852\n",
      "495 / 852\n",
      "496 / 852\n",
      "497 / 852\n",
      "498 / 852\n",
      "499 / 852\n",
      "500 / 852\n",
      "501 / 852\n",
      "502 / 852\n",
      "503 / 852\n",
      "504 / 852\n",
      "505 / 852\n",
      "506 / 852\n",
      "507 / 852\n",
      "508 / 852\n",
      "509 / 852\n",
      "510 / 852\n",
      "511 / 852\n",
      "512 / 852\n",
      "513 / 852\n",
      "514 / 852\n",
      "515 / 852\n",
      "516 / 852\n",
      "517 / 852\n",
      "518 / 852\n",
      "519 / 852\n",
      "520 / 852\n",
      "521 / 852\n",
      "522 / 852\n",
      "523 / 852\n",
      "524 / 852\n",
      "525 / 852\n",
      "526 / 852\n",
      "527 / 852\n",
      "528 / 852\n",
      "529 / 852\n",
      "530 / 852\n",
      "531 / 852\n",
      "532 / 852\n",
      "533 / 852\n",
      "534 / 852\n",
      "535 / 852\n",
      "536 / 852\n",
      "537 / 852\n",
      "538 / 852\n",
      "539 / 852\n",
      "540 / 852\n",
      "541 / 852\n",
      "542 / 852\n",
      "543 / 852\n",
      "544 / 852\n",
      "545 / 852\n",
      "546 / 852\n",
      "547 / 852\n",
      "548 / 852\n",
      "549 / 852\n",
      "550 / 852\n",
      "551 / 852\n",
      "552 / 852\n",
      "553 / 852\n",
      "554 / 852\n",
      "555 / 852\n",
      "556 / 852\n",
      "557 / 852\n",
      "558 / 852\n",
      "559 / 852\n",
      "560 / 852\n",
      "561 / 852\n",
      "562 / 852\n",
      "563 / 852\n",
      "564 / 852\n",
      "565 / 852\n",
      "566 / 852\n",
      "567 / 852\n",
      "568 / 852\n",
      "569 / 852\n",
      "570 / 852\n",
      "571 / 852\n",
      "572 / 852\n",
      "573 / 852\n",
      "574 / 852\n",
      "575 / 852\n",
      "576 / 852\n",
      "577 / 852\n",
      "578 / 852\n",
      "579 / 852\n",
      "580 / 852\n",
      "581 / 852\n",
      "582 / 852\n",
      "583 / 852\n",
      "584 / 852\n",
      "585 / 852\n",
      "586 / 852\n",
      "587 / 852\n",
      "588 / 852\n",
      "589 / 852\n",
      "590 / 852\n",
      "591 / 852\n",
      "592 / 852\n",
      "593 / 852\n",
      "594 / 852\n",
      "595 / 852\n",
      "596 / 852\n",
      "597 / 852\n",
      "598 / 852\n",
      "599 / 852\n",
      "600 / 852\n",
      "601 / 852\n",
      "602 / 852\n",
      "603 / 852\n",
      "604 / 852\n",
      "605 / 852\n",
      "606 / 852\n",
      "607 / 852\n",
      "608 / 852\n",
      "609 / 852\n",
      "610 / 852\n",
      "611 / 852\n",
      "612 / 852\n",
      "613 / 852\n",
      "614 / 852\n",
      "615 / 852\n",
      "616 / 852\n",
      "617 / 852\n",
      "618 / 852\n",
      "619 / 852\n",
      "620 / 852\n",
      "621 / 852\n",
      "622 / 852\n",
      "623 / 852\n",
      "624 / 852\n",
      "625 / 852\n",
      "626 / 852\n",
      "627 / 852\n",
      "628 / 852\n",
      "629 / 852\n",
      "630 / 852\n",
      "631 / 852\n",
      "632 / 852\n",
      "633 / 852\n",
      "634 / 852\n",
      "635 / 852\n",
      "636 / 852\n",
      "637 / 852\n",
      "638 / 852\n",
      "639 / 852\n",
      "640 / 852\n",
      "641 / 852\n",
      "642 / 852\n",
      "643 / 852\n",
      "644 / 852\n",
      "645 / 852\n",
      "646 / 852\n",
      "647 / 852\n",
      "648 / 852\n",
      "649 / 852\n",
      "650 / 852\n",
      "651 / 852\n",
      "652 / 852\n",
      "653 / 852\n",
      "654 / 852\n",
      "655 / 852\n",
      "656 / 852\n",
      "657 / 852\n",
      "658 / 852\n",
      "659 / 852\n",
      "660 / 852\n",
      "661 / 852\n",
      "662 / 852\n",
      "663 / 852\n",
      "664 / 852\n",
      "665 / 852\n",
      "666 / 852\n",
      "667 / 852\n",
      "668 / 852\n",
      "669 / 852\n",
      "670 / 852\n",
      "671 / 852\n",
      "672 / 852\n",
      "673 / 852\n",
      "674 / 852\n",
      "675 / 852\n",
      "676 / 852\n",
      "677 / 852\n",
      "678 / 852\n",
      "679 / 852\n",
      "680 / 852\n",
      "681 / 852\n",
      "682 / 852\n",
      "683 / 852\n",
      "684 / 852\n",
      "685 / 852\n",
      "686 / 852\n",
      "687 / 852\n",
      "688 / 852\n",
      "689 / 852\n",
      "690 / 852\n",
      "691 / 852\n",
      "692 / 852\n",
      "693 / 852\n",
      "694 / 852\n",
      "695 / 852\n",
      "696 / 852\n",
      "697 / 852\n",
      "698 / 852\n",
      "699 / 852\n",
      "700 / 852\n",
      "701 / 852\n",
      "702 / 852\n",
      "703 / 852\n",
      "704 / 852\n",
      "705 / 852\n",
      "706 / 852\n",
      "707 / 852\n",
      "708 / 852\n",
      "709 / 852\n",
      "710 / 852\n",
      "711 / 852\n",
      "712 / 852\n",
      "713 / 852\n",
      "714 / 852\n",
      "715 / 852\n",
      "716 / 852\n",
      "717 / 852\n",
      "718 / 852\n",
      "719 / 852\n",
      "720 / 852\n",
      "721 / 852\n",
      "722 / 852\n",
      "723 / 852\n",
      "724 / 852\n",
      "725 / 852\n",
      "726 / 852\n",
      "727 / 852\n",
      "728 / 852\n",
      "729 / 852\n",
      "730 / 852\n",
      "731 / 852\n",
      "732 / 852\n",
      "733 / 852\n",
      "734 / 852\n",
      "735 / 852\n",
      "736 / 852\n",
      "737 / 852\n",
      "738 / 852\n",
      "739 / 852\n",
      "740 / 852\n",
      "741 / 852\n",
      "742 / 852\n",
      "743 / 852\n",
      "744 / 852\n",
      "745 / 852\n",
      "746 / 852\n",
      "747 / 852\n",
      "748 / 852\n",
      "749 / 852\n",
      "750 / 852\n",
      "751 / 852\n",
      "752 / 852\n",
      "753 / 852\n",
      "754 / 852\n",
      "755 / 852\n",
      "756 / 852\n",
      "757 / 852\n",
      "758 / 852\n",
      "759 / 852\n",
      "760 / 852\n",
      "761 / 852\n",
      "762 / 852\n",
      "763 / 852\n",
      "764 / 852\n",
      "765 / 852\n",
      "766 / 852\n",
      "767 / 852\n",
      "768 / 852\n",
      "769 / 852\n",
      "770 / 852\n",
      "771 / 852\n",
      "772 / 852\n",
      "773 / 852\n",
      "774 / 852\n",
      "775 / 852\n",
      "776 / 852\n",
      "777 / 852\n",
      "778 / 852\n",
      "779 / 852\n",
      "780 / 852\n",
      "781 / 852\n",
      "782 / 852\n",
      "783 / 852\n",
      "784 / 852\n",
      "785 / 852\n",
      "786 / 852\n",
      "787 / 852\n",
      "788 / 852\n",
      "789 / 852\n",
      "790 / 852\n",
      "791 / 852\n",
      "792 / 852\n",
      "793 / 852\n",
      "794 / 852\n",
      "795 / 852\n",
      "796 / 852\n",
      "797 / 852\n",
      "798 / 852\n",
      "799 / 852\n",
      "800 / 852\n",
      "801 / 852\n",
      "802 / 852\n",
      "803 / 852\n",
      "804 / 852\n",
      "805 / 852\n",
      "806 / 852\n",
      "807 / 852\n",
      "808 / 852\n",
      "809 / 852\n",
      "810 / 852\n",
      "811 / 852\n",
      "812 / 852\n",
      "813 / 852\n",
      "814 / 852\n",
      "815 / 852\n",
      "816 / 852\n",
      "817 / 852\n",
      "818 / 852\n",
      "819 / 852\n",
      "820 / 852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821 / 852\n",
      "822 / 852\n",
      "823 / 852\n",
      "824 / 852\n",
      "825 / 852\n",
      "826 / 852\n",
      "827 / 852\n",
      "828 / 852\n",
      "829 / 852\n",
      "830 / 852\n",
      "831 / 852\n",
      "832 / 852\n",
      "833 / 852\n",
      "834 / 852\n",
      "835 / 852\n",
      "836 / 852\n",
      "837 / 852\n",
      "838 / 852\n",
      "839 / 852\n",
      "840 / 852\n",
      "841 / 852\n",
      "842 / 852\n",
      "843 / 852\n",
      "844 / 852\n",
      "845 / 852\n",
      "846 / 852\n",
      "847 / 852\n",
      "848 / 852\n",
      "849 / 852\n",
      "850 / 852\n",
      "851 / 852\n"
     ]
    }
   ],
   "source": [
    "wordset = set()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for line in X_train:\n",
    "    print(i, \"/\", len(X_train))\n",
    "    i +=1\n",
    "    line[0] = preprocessing(line[0])\n",
    "    line[1] = preprocessing(line[1])\n",
    "    for word in line[0]:\n",
    "        wordset.add(word)\n",
    "    for word in line[1]:\n",
    "        wordset.add(word)\n",
    "\n",
    "i = 0\n",
    "for line in X_test:\n",
    "    print(i, \"/\", len(X_test))\n",
    "    i +=1\n",
    "    line[0] = preprocessing(line[0])\n",
    "    line[1] = preprocessing(line[1])\n",
    "    for word in line[0]:\n",
    "        wordset.add(word)\n",
    "    for word in line[1]:\n",
    "        wordset.add(word)\n",
    "\n",
    "word_embedding, words_dict = load_embedding(embedding_file_path, wordset, embedding_dim)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "no_word_vector = np.zeros(embedding_dim)\n",
    "\n",
    "for line in X_train:\n",
    "\n",
    "    sentence = []\n",
    "    for i in range(max_sen_len):\n",
    "        if i < len(line[0]) and line[0][i] in words_dict:\n",
    "            sentence.append(word_embedding[words_dict[line[0][i]]])\n",
    "        else :\n",
    "            sentence.append(no_word_vector)\n",
    "    line[0] = np.array(sentence)\n",
    "\n",
    "    sentence = []\n",
    "    for i in range(max_sen_len):\n",
    "        if i < len(line[1]) and line[1][i] in words_dict:\n",
    "            sentence.append(word_embedding[words_dict[line[1][i]]])\n",
    "        else :\n",
    "            sentence.append(no_word_vector)\n",
    "    line[1] = np.array(sentence)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "argmax_y_train = []\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == '0':\n",
    "        argmax_y_train.append(0)\n",
    "    else :\n",
    "        argmax_y_train.append(1)\n",
    "\n",
    "for line in X_test:\n",
    "    sentence = []\n",
    "    for i in range(max_sen_len):\n",
    "        if i < len(line[0]) and line[0][i] in words_dict:\n",
    "            sentence.append(word_embedding[words_dict[line[0][i]]])\n",
    "        else :\n",
    "            sentence.append(no_word_vector)\n",
    "    line[0] = np.array(sentence)\n",
    "    sentence = []\n",
    "    for i in range(max_sen_len):\n",
    "        if i < len(line[1]) and line[1][i] in words_dict:\n",
    "            sentence.append(word_embedding[words_dict[line[1][i]]])\n",
    "        else :\n",
    "            sentence.append(no_word_vector)\n",
    "    line[1] = np.array(sentence)\n",
    "\n",
    "y_train = []\n",
    "for i in range(len(argmax_y_train)):\n",
    "    if argmax_y_train[i] == 0:\n",
    "        y_train.append([1, 0])\n",
    "    else :\n",
    "        y_train.append([0, 1])\n",
    "\n",
    "\n",
    "# In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79b31443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.20437 ,  0.16431 ,  0.041794, ..., -0.34007 , -0.077146,\n",
       "          -0.084089],\n",
       "         [-0.11234 ,  0.029688, -0.25461 , ..., -0.30062 , -0.45811 ,\n",
       "          -0.088131],\n",
       "         [-0.11935 ,  0.096194,  0.14377 , ...,  0.36304 , -0.18931 ,\n",
       "           0.21588 ],\n",
       "         ...,\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ]]),\n",
       "  array([[ 0.04656  ,  0.21318  , -0.0074364, ...,  0.0090611, -0.20989  ,\n",
       "           0.053913 ],\n",
       "         [-0.10292  , -0.0040756, -0.087242 , ...,  0.06255  ,  0.21399  ,\n",
       "           0.035232 ],\n",
       "         [-0.063767 ,  0.14054  ,  0.18494  , ..., -0.020575 , -0.52352  ,\n",
       "           0.059777 ],\n",
       "         ...,\n",
       "         [-0.20969  ,  0.18073  , -0.16401  , ..., -0.33588  , -0.0595   ,\n",
       "          -0.28637  ],\n",
       "         [ 0.029215 ,  0.10225  ,  0.14988  , ...,  0.21649  , -0.20815  ,\n",
       "           0.25712  ],\n",
       "         [ 0.096017 ,  0.25275  , -0.16765  , ..., -0.52869  , -0.45494  ,\n",
       "           0.0014049]])],\n",
       " [array([[-0.060801,  0.19785 ,  0.85434 , ...,  0.47852 ,  0.032239,\n",
       "           0.59133 ],\n",
       "         [-0.016007,  0.48266 ,  0.2996  , ..., -0.037008,  0.14211 ,\n",
       "           0.074715],\n",
       "         [-0.25756 , -0.057132, -0.6719  , ..., -0.16043 ,  0.046744,\n",
       "          -0.070621],\n",
       "         ...,\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ]]),\n",
       "  array([[ 0.44361 , -0.24175 ,  0.23657 , ...,  0.18575 , -0.29561 ,\n",
       "          -0.19987 ],\n",
       "         [ 0.13836 , -0.26681 ,  0.29052 , ..., -0.15518 ,  0.027872,\n",
       "           0.39141 ],\n",
       "         [-0.060801,  0.19785 ,  0.85434 , ...,  0.47852 ,  0.032239,\n",
       "           0.59133 ],\n",
       "         ...,\n",
       "         [-0.36898 ,  0.89703 ,  0.34799 , ..., -0.39875 , -0.82992 ,\n",
       "           0.1757  ],\n",
       "         [ 0.35923 , -0.075825, -0.18575 , ..., -0.50612 ,  0.43401 ,\n",
       "          -0.064001],\n",
       "         [ 0.14919 , -0.04861 ,  0.20387 , ..., -0.13497 ,  0.64464 ,\n",
       "          -0.61623 ]])],\n",
       " [array([[ 0.18562  ,  0.10207  ,  0.1642   , ..., -0.10026  , -0.66099  ,\n",
       "           0.48371  ],\n",
       "         [-0.76078  ,  0.12378  , -0.27611  , ...,  0.0026165, -0.76359  ,\n",
       "          -0.19649  ],\n",
       "         [-0.068696 , -0.39391  ,  0.45735  , ...,  0.27555  , -0.65322  ,\n",
       "           0.18833  ],\n",
       "         ...,\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "           0.       ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "           0.       ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "           0.       ]]),\n",
       "  array([[-0.043852 , -0.37008  ,  0.055428 , ..., -1.0012   ,  0.1049   ,\n",
       "           0.056632 ],\n",
       "         [-0.19563  ,  0.48127  , -0.2467   , ..., -0.87195  ,  0.061883 ,\n",
       "          -0.1718   ],\n",
       "         [-0.038226 ,  0.23505  ,  0.0094661, ..., -0.7121   , -0.38866  ,\n",
       "          -0.2271   ],\n",
       "         ...,\n",
       "         [ 0.27547  , -0.33215  ,  0.35655  , ...,  0.63216  , -0.91011  ,\n",
       "           0.5916   ],\n",
       "         [ 0.087077 , -0.65279  ,  0.26419  , ...,  0.099651 , -0.54738  ,\n",
       "           0.47105  ],\n",
       "         [-0.46486  ,  0.080407 , -0.19959  , ...,  0.026038 ,  0.2065   ,\n",
       "           0.53307  ]])],\n",
       " [array([[ 0.049177,  0.056631,  0.36762 , ..., -0.41919 ,  0.37843 ,\n",
       "           0.41108 ],\n",
       "         [-0.13292 ,  0.16985 , -0.1436  , ..., -0.23778 ,  0.14766 ,\n",
       "           0.62902 ],\n",
       "         [-0.19563 ,  0.48127 , -0.2467  , ..., -0.87195 ,  0.061883,\n",
       "          -0.1718  ],\n",
       "         ...,\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ]]),\n",
       "  array([[ 0.43665 ,  0.18793 , -0.17022 , ...,  0.032894, -0.52144 ,\n",
       "           0.22295 ],\n",
       "         [ 0.049177,  0.056631,  0.36762 , ..., -0.41919 ,  0.37843 ,\n",
       "           0.41108 ],\n",
       "         [-0.24135 ,  0.15132 ,  0.016839, ...,  0.44316 , -0.93459 ,\n",
       "           0.40802 ],\n",
       "         ...,\n",
       "         [-0.37765 , -0.74258 , -0.02659 , ..., -0.10374 ,  0.095398,\n",
       "           0.25213 ],\n",
       "         [-0.22746 , -0.13658 , -0.38997 , ..., -0.18444 , -0.38228 ,\n",
       "           0.55346 ],\n",
       "         [ 0.2735  , -0.29025 , -0.031476, ...,  0.19898 ,  0.050869,\n",
       "           0.012133]])],\n",
       " [array([[ 0.049177,  0.056631,  0.36762 , ..., -0.41919 ,  0.37843 ,\n",
       "           0.41108 ],\n",
       "         [ 0.32183 ,  0.015081,  0.20434 , ...,  0.29599 ,  0.12607 ,\n",
       "           0.47198 ],\n",
       "         [-0.62333 , -0.42434 , -0.035321, ..., -0.13613 ,  0.09868 ,\n",
       "           0.609   ],\n",
       "         ...,\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ],\n",
       "         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "           0.      ]]),\n",
       "  array([[-0.37897  , -0.0056951, -0.050075 , ...,  0.86385  ,  0.095379 ,\n",
       "           0.2151   ],\n",
       "         [-0.21093  ,  0.51757  ,  0.042662 , ...,  0.18811  , -0.033099 ,\n",
       "          -0.21241  ],\n",
       "         [ 0.049177 ,  0.056631 ,  0.36762  , ..., -0.41919  ,  0.37843  ,\n",
       "           0.41108  ],\n",
       "         ...,\n",
       "         [-0.29712  ,  0.094049 , -0.096662 , ...,  0.059717 , -0.22853  ,\n",
       "           0.29602  ],\n",
       "         [ 0.053202 ,  0.32654  , -0.01127  , ..., -0.39005  , -0.1441   ,\n",
       "           0.42055  ],\n",
       "         [ 0.19805  ,  0.11574  ,  0.031307 , ..., -0.37545  , -0.40642  ,\n",
       "           0.088813 ]])]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62f2625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0], [0, 1], [1, 0], [0, 1], [1, 0]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02482bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape  Counter({0: 426, 1: 426})\n",
      "shape\n",
      "(852, 2, 50, 300)\n",
      "(852, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATASET RE-SAMPLING\n",
    "'''\n",
    "print(\"Original dataset shape \", Counter(argmax_y_train))\n",
    "\n",
    "\n",
    "print(\"shape\")\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a88126a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split in train and test set\n",
    "'''\n",
    "\n",
    "test_percentage = 0.20\n",
    "\n",
    "train_X = np.array(X_train[int(test_percentage*len(X_train)):])\n",
    "train_X_lenght = np.array(X_train_lenght[int(test_percentage*len(X_train_lenght)):])\n",
    "train_y = np.array(y_train[int(test_percentage*len(y_train)):])\n",
    "test_X = np.array(X_train[:int(test_percentage*len(X_train))])\n",
    "test_X_lenght = np.array(X_test_lenght[:int(test_percentage*len(X_test_lenght))])\n",
    "test_y = np.array(y_train[:int(test_percentage*len(y_train))])\n",
    "\n",
    "train_dataset = [train_X, train_X_lenght, train_y, test_X, test_X_lenght, test_y]\n",
    "\n",
    "\n",
    "test_dataset = [X_test, X_test_lenght]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "592765f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape\n",
      "(682, 2, 50, 300)\n",
      "(682, 2)\n",
      "(170, 2, 50, 300)\n",
      "(170, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape\")\n",
    "print(np.shape(train_X))\n",
    "print(np.shape(train_y))\n",
    "print(np.shape(test_X))\n",
    "print(np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c8073ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape  Counter({0: 426, 1: 426})\n"
     ]
    }
   ],
   "source": [
    "print(\"Original dataset shape \", Counter(argmax_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4e319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60faa17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1073ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b779b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dataset\n",
      "train dataset done\n",
      "test dataset done\n"
     ]
    }
   ],
   "source": [
    "# In[12]:\n",
    "\n",
    "'''\n",
    "Save dataset\n",
    "'''\n",
    "print(\"Save dataset\")\n",
    "\n",
    "with open(train_dataset_file_path, 'wb') as f:\n",
    "    pickle.dump(train_dataset, f, protocol=4)\n",
    "print(\"train dataset done\")\n",
    "\n",
    "with open(test_dataset_file_path, 'wb') as f:\n",
    "    pickle.dump(test_dataset, f, protocol=4)\n",
    "print(\"test dataset done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e191351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, max_sen_len, class_num, embedding_dim, hidden_size):\n",
    "\n",
    "        self.max_sen_len = max_sen_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.class_num = class_num\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with tf.name_scope('input'):\n",
    "            self.x1 = tf.placeholder(tf.float32, [None, self.max_sen_len, self.embedding_dim], name=\"x1\")\n",
    "            self.x2 = tf.placeholder(tf.float32, [None, self.max_sen_len, self.embedding_dim], name=\"x2\")\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.class_num], name=\"y\")\n",
    "\n",
    "        with tf.name_scope('weights'):\n",
    "            self.weights = {\n",
    "                'q_1_to_2': tf.Variable(tf.random_uniform([4*embedding_dim, self.hidden_size], -0.01, 0.01)),\n",
    "\n",
    "                'p_1_to_2': tf.Variable(tf.random_uniform([self.hidden_size, 1], -0.01, 0.01)),\n",
    "\n",
    "                'z': tf.Variable(tf.random_uniform([2*self.embedding_dim+self.hidden_size, self.hidden_size], -0.01, 0.01)),\n",
    "\n",
    "                'f': tf.Variable(tf.random_uniform([self.hidden_size, self.class_num], -0.01, 0.01)),\n",
    "            }\n",
    "\n",
    "        with tf.name_scope('biases'):\n",
    "            self.biases = {\n",
    "                'q_1_to_2': tf.Variable(tf.random_uniform([self.hidden_size], -0.01, 0.01)),\n",
    "\n",
    "                'p_1_to_2': tf.Variable(tf.random_uniform([1], -0.01, 0.01)),\n",
    "\n",
    "                'z': tf.Variable(tf.random_uniform([self.hidden_size], -0.01, 0.01)),\n",
    "\n",
    "                'f': tf.Variable(tf.random_uniform([self.class_num], -0.01, 0.01)),\n",
    "            }\n",
    "\n",
    "    def inter_attention(self):\n",
    "        \n",
    "        x1_shape = tf.shape(self.x1)\n",
    "        x2_shape = tf.shape(self.x2)\n",
    "\n",
    "        x1_reshape = tf.reshape(self.x1, [-1, self.embedding_dim, 1])\n",
    "        ones = tf.ones([x1_shape[0]*self.max_sen_len, 1, self.max_sen_len])\n",
    "        x1_increase = tf.matmul(x1_reshape, ones)\n",
    "        x1_increase = tf.transpose(x1_increase, perm=[0, 2, 1])\n",
    "        x1_increase = tf.reshape(x1_increase, [-1, self.max_sen_len*self.max_sen_len, self.embedding_dim])\n",
    "\n",
    "        x2_reshape = tf.reshape(self.x2, [-1, self.embedding_dim, 1])\n",
    "        ones = tf.ones([x2_shape[0]*self.max_sen_len, 1, self.max_sen_len])\n",
    "        x2_increase = tf.matmul(x2_reshape, ones)\n",
    "        x2_increase = tf.transpose(x2_increase, perm=[0, 2, 1])\n",
    "        x2_increase = tf.reshape(x2_increase, [-1, self.max_sen_len, self.max_sen_len, self.embedding_dim])\n",
    "        x2_increase = tf.transpose(x2_increase, perm=[0, 2, 1, 3])\n",
    "        x2_increase = tf.reshape(x2_increase, [-1, self.max_sen_len*self.max_sen_len, self.embedding_dim])\n",
    "\n",
    "        concat = tf.concat([x1_increase, x2_increase], axis=-1)\n",
    "        concat = tf.reshape(concat, [-1, 2*self.embedding_dim])\n",
    "\n",
    "        dot = tf.multiply(x1_increase, x2_increase)\n",
    "        dot = tf.reshape(dot, [-1, self.embedding_dim])\n",
    "\n",
    "        substract = tf.math.subtract(x1_increase, x2_increase)\n",
    "        substract = tf.reshape(substract, [-1, self.embedding_dim])\n",
    "\n",
    "        s_1_to_2 = tf.nn.relu(tf.matmul(tf.concat([concat, dot, substract], axis=-1), self.weights['q_1_to_2']) + self.biases['q_1_to_2'])\n",
    "        s_1_to_2 = tf.matmul(s_1_to_2, self.weights['p_1_to_2']) + self.biases['p_1_to_2']\n",
    "        s_1_to_2 = tf.reshape(s_1_to_2, [-1, self.max_sen_len, self.max_sen_len])\n",
    "\n",
    "        a_1 = tf.reshape(tf.nn.softmax(tf.reduce_max(s_1_to_2, axis=-1), axis=-1), [-1, 1, self.max_sen_len])\n",
    "\n",
    "        self.v_a_1_to_2 = tf.reshape(tf.matmul(a_1, self.x1), [-1, self.embedding_dim])\n",
    "\n",
    "        a_2 = tf.reshape(tf.nn.softmax(tf.reduce_max(tf.transpose(s_1_to_2, perm=[0, 2, 1]), axis=-1), axis=-1), [-1, 1, self.max_sen_len])\n",
    "\n",
    "        self.v_a_2_to_1 = tf.reshape(tf.matmul(a_2, self.x2), [-1, self.embedding_dim])\n",
    "\n",
    "        self.v_a = tf.concat([self.v_a_1_to_2, self.v_a_2_to_1], axis=-1)\n",
    "\n",
    "    def long_short_memory_encoder(self):\n",
    "\n",
    "        lstm_cell = tf.keras.layers.LSTMCell(self.hidden_size)\n",
    "        LSTM_layer = tf.keras.layers.RNN(lstm_cell)\n",
    "        self.v_c = LSTM_layer(tf.concat([self.x1, self.x2], axis=1))\n",
    "\n",
    "    def prediction(self):\n",
    "\n",
    "        v = tf.concat([self.v_a, self.v_c], -1)\n",
    "        v = tf.nn.relu(tf.matmul(v, self.weights['z']) + self.biases['z'])\n",
    "\n",
    "        self.scores = tf.nn.softmax((tf.matmul(v, self.weights['f']) + self.biases['f']), axis=-1)\n",
    "\n",
    "        self.predictions = tf.argmax(self.scores, -1, name=\"predictions\")\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        self.inter_attention()\n",
    "        self.long_short_memory_encoder()\n",
    "        self.prediction()\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                #labels=tf.argmax(self.y, -1),\n",
    "                labels=self.y,\n",
    "                logits=self.scores\n",
    "            )\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.y, -1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            self.c_matrix = tf.confusion_matrix(labels = tf.argmax(self.y, -1), predictions = self.predictions, name=\"c_matrix\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba36f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore Data\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "data_directory = \"./\"\n",
    "backup_directory = \"../Models/\"\n",
    "\n",
    "dataset_file_path = data_directory+\"/train_dataset_300\"\n",
    "\n",
    "print(\"Restore Data\")\n",
    "\n",
    "with open(dataset_file_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aea1b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET : (6,)\n",
      "train_X : (682, 2, 50, 300)\n",
      "train_y : (682, 2)\n",
      "test_X : (170, 2, 50, 300)\n",
      "test_y : (170, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET :\", np.shape(dataset))\n",
    "print(\"train_X :\", np.shape(dataset[0]))\n",
    "print(\"train_y :\", np.shape(dataset[2]))\n",
    "print(\"test_X :\", np.shape(dataset[3]))\n",
    "print(\"test_y :\", np.shape(dataset[5]))\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "train_X = np.array(dataset[0])\n",
    "#train_X_lenght = np.array(dataset[1])\n",
    "train_y = np.array(dataset[2])\n",
    "test_X = np.array(dataset[3])\n",
    "#test_X_lenght = np.array(dataset[4])\n",
    "test_y = np.array(dataset[5])\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "n_class = 2\n",
    "embedding_dim = 300\n",
    "max_sen_len = 50\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "test_batch_size = 200\n",
    "num_epochs = 10\n",
    "evaluate_every = 500\n",
    "\n",
    "nb_batch_per_epoch = int(len(train_X)/batch_size+1)\n",
    "nb_batch_per_epoch_test = int(len(test_X)/test_batch_size+1)\n",
    "\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07cca289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad009b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation\n",
      "Model construction\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Model construction 2\n",
      "Model construction 3\n",
      "Model construction 4\n",
      "Model construction 5\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:12.171965: step 1/70, loss 0.692853, acc 0.56\n",
      "[[56  0]\n",
      " [44  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:15.209405: step 2/70, loss 0.693465, acc 0.47\n",
      "[[47  0]\n",
      " [53  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:18.036974: step 3/70, loss 0.692842, acc 0.52\n",
      "[[52  0]\n",
      " [48  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:20.899291: step 4/70, loss 0.693162, acc 0.49\n",
      "[[49  0]\n",
      " [51  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:23.764090: step 5/70, loss 0.692752, acc 0.5\n",
      "[[50  0]\n",
      " [50  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:26.619546: step 6/70, loss 0.693063, acc 0.45\n",
      "[[45  0]\n",
      " [55  0]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:28.989549: step 7/70, loss 0.692082, acc 0.617284\n",
      "[[40  2]\n",
      " [29 10]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:32.018580: step 8/70, loss 0.691799, acc 0.72\n",
      "[[42  3]\n",
      " [25 30]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:34.829867: step 9/70, loss 0.691282, acc 0.71\n",
      "[[41 10]\n",
      " [19 30]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:37.689879: step 10/70, loss 0.69023, acc 0.72\n",
      "[[40  9]\n",
      " [19 32]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:40.531041: step 11/70, loss 0.691297, acc 0.56\n",
      "[[40  7]\n",
      " [37 16]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:43.419453: step 12/70, loss 0.687972, acc 0.8\n",
      "[[40  9]\n",
      " [11 40]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:46.292560: step 13/70, loss 0.686202, acc 0.75\n",
      "[[49 10]\n",
      " [15 26]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:48.661241: step 14/70, loss 0.684938, acc 0.654321\n",
      "[[38  2]\n",
      " [26 15]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:51.614868: step 15/70, loss 0.682179, acc 0.69\n",
      "[[52  2]\n",
      " [29 17]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:54.476374: step 16/70, loss 0.679153, acc 0.67\n",
      "[[50  3]\n",
      " [30 17]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:48:57.382672: step 17/70, loss 0.682649, acc 0.56\n",
      "[[47  0]\n",
      " [44  9]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:00.303896: step 18/70, loss 0.672149, acc 0.64\n",
      "[[52  1]\n",
      " [35 12]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:03.244625: step 19/70, loss 0.670538, acc 0.68\n",
      "[[44  3]\n",
      " [29 24]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:06.143203: step 20/70, loss 0.668453, acc 0.7\n",
      "[[45  5]\n",
      " [25 25]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:08.519122: step 21/70, loss 0.659317, acc 0.765432\n",
      "[[30  7]\n",
      " [12 32]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:11.557730: step 22/70, loss 0.6519, acc 0.75\n",
      "[[38 13]\n",
      " [12 37]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:14.528490: step 23/70, loss 0.648258, acc 0.75\n",
      "[[36 15]\n",
      " [10 39]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:17.441332: step 24/70, loss 0.632984, acc 0.72\n",
      "[[41  5]\n",
      " [23 31]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:20.381990: step 25/70, loss 0.643423, acc 0.69\n",
      "[[44  6]\n",
      " [25 25]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:23.297075: step 26/70, loss 0.612085, acc 0.77\n",
      "[[49  3]\n",
      " [20 28]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:26.258556: step 27/70, loss 0.609821, acc 0.75\n",
      "[[45  6]\n",
      " [19 30]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:28.667670: step 28/70, loss 0.591729, acc 0.777778\n",
      "[[27 12]\n",
      " [ 6 36]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:31.709360: step 29/70, loss 0.574742, acc 0.8\n",
      "[[36 12]\n",
      " [ 8 44]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:34.642784: step 30/70, loss 0.564122, acc 0.77\n",
      "[[35 10]\n",
      " [13 42]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:37.625700: step 31/70, loss 0.526974, acc 0.87\n",
      "[[44  8]\n",
      " [ 5 43]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:40.534772: step 32/70, loss 0.5016, acc 0.88\n",
      "[[45  5]\n",
      " [ 7 43]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:43.453092: step 33/70, loss 0.575187, acc 0.75\n",
      "[[38 12]\n",
      " [13 37]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:46.315483: step 34/70, loss 0.47026, acc 0.87\n",
      "[[46  9]\n",
      " [ 4 41]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:48.729434: step 35/70, loss 0.4871, acc 0.864198\n",
      "[[37  4]\n",
      " [ 7 33]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:51.795505: step 36/70, loss 0.464846, acc 0.85\n",
      "[[39  6]\n",
      " [ 9 46]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:54.524061: step 37/70, loss 0.465903, acc 0.86\n",
      "[[43  5]\n",
      " [ 9 43]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:49:57.378372: step 38/70, loss 0.43404, acc 0.93\n",
      "[[42  7]\n",
      " [ 0 51]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:00.118386: step 39/70, loss 0.477313, acc 0.82\n",
      "[[44  5]\n",
      " [13 38]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:02.749184: step 40/70, loss 0.450113, acc 0.9\n",
      "[[49  2]\n",
      " [ 8 41]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:05.709121: step 41/70, loss 0.465865, acc 0.84\n",
      "[[47 12]\n",
      " [ 4 37]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:08.006641: step 42/70, loss 0.416719, acc 0.91358\n",
      "[[36  4]\n",
      " [ 3 38]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:12.075932: step 43/70, loss 0.406587, acc 0.91\n",
      "[[37  2]\n",
      " [ 7 54]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:15.193536: step 44/70, loss 0.408216, acc 0.91\n",
      "[[50  6]\n",
      " [ 3 41]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:18.077588: step 45/70, loss 0.386025, acc 0.93\n",
      "[[56  5]\n",
      " [ 2 37]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:21.099993: step 46/70, loss 0.393987, acc 0.94\n",
      "[[57  3]\n",
      " [ 3 37]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:24.072144: step 47/70, loss 0.441906, acc 0.85\n",
      "[[38  5]\n",
      " [10 47]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:26.936933: step 48/70, loss 0.424612, acc 0.9\n",
      "[[44  1]\n",
      " [ 9 46]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:29.340313: step 49/70, loss 0.37973, acc 0.962963\n",
      "[[35  1]\n",
      " [ 2 43]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:32.451878: step 50/70, loss 0.395037, acc 0.92\n",
      "[[44  8]\n",
      " [ 0 48]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:35.339871: step 51/70, loss 0.36585, acc 0.96\n",
      "[[51  3]\n",
      " [ 1 45]]\n",
      "Model construction 8\n",
      "Model construction 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model construction 10\n",
      "2022-03-23T16:50:38.203067: step 52/70, loss 0.373561, acc 0.94\n",
      "[[47  1]\n",
      " [ 5 47]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:41.027762: step 53/70, loss 0.408467, acc 0.91\n",
      "[[48  4]\n",
      " [ 5 43]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:43.939043: step 54/70, loss 0.36943, acc 0.96\n",
      "[[55  2]\n",
      " [ 2 41]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:46.858492: step 55/70, loss 0.388421, acc 0.94\n",
      "[[36  3]\n",
      " [ 3 58]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:49.253012: step 56/70, loss 0.358738, acc 0.962963\n",
      "[[38  1]\n",
      " [ 2 40]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:52.286726: step 57/70, loss 0.348772, acc 0.97\n",
      "[[43  2]\n",
      " [ 1 54]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:55.007054: step 58/70, loss 0.360809, acc 0.95\n",
      "[[44  4]\n",
      " [ 1 51]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:50:58.002902: step 59/70, loss 0.360601, acc 0.96\n",
      "[[52  3]\n",
      " [ 1 44]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:01.010145: step 60/70, loss 0.355585, acc 0.95\n",
      "[[47  4]\n",
      " [ 1 48]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:03.970492: step 61/70, loss 0.351797, acc 0.97\n",
      "[[47  1]\n",
      " [ 2 50]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:06.843494: step 62/70, loss 0.351631, acc 0.98\n",
      "[[52  2]\n",
      " [ 0 46]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:09.329409: step 63/70, loss 0.376876, acc 0.938272\n",
      "[[38  2]\n",
      " [ 3 38]]\n",
      "Model construction 6\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:12.472582: step 64/70, loss 0.337155, acc 0.98\n",
      "[[45  1]\n",
      " [ 1 53]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:15.581439: step 65/70, loss 0.318566, acc 1\n",
      "[[55  0]\n",
      " [ 0 45]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:18.476468: step 66/70, loss 0.35842, acc 0.96\n",
      "[[44  4]\n",
      " [ 0 52]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:21.337263: step 67/70, loss 0.342161, acc 0.98\n",
      "[[41  2]\n",
      " [ 0 57]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:24.242216: step 68/70, loss 0.339956, acc 0.98\n",
      "[[50  2]\n",
      " [ 0 48]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:27.100142: step 69/70, loss 0.373697, acc 0.94\n",
      "[[46  4]\n",
      " [ 2 48]]\n",
      "Model construction 8\n",
      "Model construction 9\n",
      "Model construction 10\n",
      "2022-03-23T16:51:29.496183: step 70/70, loss 0.33676, acc 0.987654\n",
      "[[47  0]\n",
      " [ 1 33]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_config = tf.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement\n",
    "    )\n",
    "    session_config.gpu_options.allow_growth = False\n",
    "    session_config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "    sess = tf.Session(config=session_config)\n",
    "\n",
    "    with sess.as_default():\n",
    "\n",
    "        print(\"Model creation\")\n",
    "\n",
    "        model = Model(\n",
    "        max_sen_len = max_sen_len,\n",
    "        embedding_dim = embedding_dim,\n",
    "        class_num = n_class,\n",
    "        hidden_size = hidden_size\n",
    "        )\n",
    "\n",
    "        print(\"Model construction\")\n",
    "\n",
    "        model.build_model()\n",
    "        print(\"Model construction 2\")\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        print(\"Model construction 3\")\n",
    "        timestamp = str(int(time.time()))\n",
    "        checkpoint_dir = os.path.abspath(backup_directory+timestamp)\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model_dpc\")\n",
    "        print(\"Model construction 4\")\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        best_accuracy = 0.\n",
    "        predict_round = 0\n",
    "        print(\"Model construction 5\")\n",
    "        for epoch in range(num_epochs):\n",
    "            indices = np.arange(len(train_X))\n",
    "            np.random.shuffle(indices)\n",
    "            train_X = train_X[indices]\n",
    "            train_y = train_y[indices]\n",
    "            print(\"Model construction 6\")\n",
    "\n",
    "            for batch in range(nb_batch_per_epoch):\n",
    "                print(\"Model construction 8\")\n",
    "                idx_min = batch * batch_size\n",
    "                idx_max = min((batch+1) * batch_size, len(train_X)-1)\n",
    "                x1 = train_X[idx_min:idx_max, 0]\n",
    "                x2 = train_X[idx_min:idx_max, 1]\n",
    "                y = train_y[idx_min:idx_max]\n",
    "                # normalized batch :\n",
    "                print(\"Model construction 9\")\n",
    "                feed_dict = {\n",
    "                    model.x1: x1,\n",
    "                    model.x2: x2,\n",
    "                    model.y: y,\n",
    "                    #model.class_weights: class_weights,\n",
    "                    #model.class_weights_accuracy: class_weights_accuracy,\n",
    "                }\n",
    "\n",
    "                _, step, loss, accuracy, c_matrix = sess.run(\n",
    "                    [train_op, global_step, model.loss, model.accuracy, model.c_matrix], \n",
    "                    feed_dict=feed_dict)\n",
    "                print(\"Model construction 10\")\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}/{}, loss {:g}, acc {:g}\".format(time_str, step, num_epochs*nb_batch_per_epoch, loss, accuracy))\n",
    "                print(c_matrix)\n",
    "\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % evaluate_every == 0:\n",
    "                    predict_round += 1\n",
    "                    print(\"\\nEvaluation round %d:\" % (predict_round))\n",
    "                    print(\"Model construction 7\")\n",
    "                    indices = np.arange(len(test_X))\n",
    "                    np.random.shuffle(indices)\n",
    "                    test_X = test_X[indices]\n",
    "                    test_y = test_y[indices]\n",
    "\n",
    "                    accuracy = 0\n",
    "                    c_matrix = np.zeros((n_class, n_class))\n",
    "\n",
    "                    for test_batch in range(nb_batch_per_epoch_test):\n",
    "                        idx_min = test_batch * test_batch_size\n",
    "                        idx_max = min((test_batch+1) * test_batch_size, len(test_X)-1)\n",
    "                        \n",
    "                        x1 = test_X[idx_min:idx_max, 0]\n",
    "                        x2 = test_X[idx_min:idx_max, 1]\n",
    "\n",
    "                        y = test_y[idx_min:idx_max]\n",
    "\n",
    " \n",
    "                                \n",
    "                        feed_dict = {\n",
    "                            model.x1: x1,\n",
    "                            model.x2: x2,\n",
    "                            model.y: y,\n",
    "                            #model.class_weights: class_weights,\n",
    "                            #model.class_weights_accuracy: class_weights_accuracy,\n",
    "                        }\n",
    "\n",
    "                        batch_accuracy, batch_c_matrix = sess.run([model.accuracy, model.c_matrix], feed_dict=feed_dict)\n",
    "                        accuracy = accuracy + batch_accuracy\n",
    "                        c_matrix = np.add(c_matrix, batch_c_matrix)\n",
    "\n",
    "                    accuracy = accuracy/nb_batch_per_epoch_test\n",
    "                    print(\"Test acc {:g}\".format(accuracy))\n",
    "                    print(\"C_matrix \", c_matrix)\n",
    "\n",
    "                    if accuracy >= best_accuracy: \n",
    "                        best_accuracy = accuracy\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dcd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ed835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
